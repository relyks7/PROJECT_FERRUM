BDH-GPU Kernel Manifest (generated by Gemini)
| Priority | Kernel Name | Forward/Backward | Rationale for BDH-GPU |
| :--- | :--- | :--- | :--- |
| Tier 1 (done) | embedding | Both | The Input Gateway. This is the non-negotiable first layer of the model. It maps input token IDs to their vector representations, which become the initial X vector in the BDH-GPU update loop. Its backward pass (scatter_add) is essential for training the token embeddings. |
| Tier 1 (done) | crossentropy_loss | Both | The Learning Engine. This is how the model calculates its error and begins the backpropagation process. For numerical stability, this must be a fused kernel that combines log_softmax with the negative log-likelihood calculation. |
| Tier 2 (done) | softmax | Both | The Output Projector. This is required as the final layer before the loss function to convert the model's output logits into a valid probability distribution over the vocabulary for next-token prediction. It must be a fused, numerically stable kernel. |
| Tier 2 (done) | layernorm | Both | The Stability
 Engine. While not the central theoretical component of the BDH-GPU's recurrent loop, Layer Normalization (or a similar normalization scheme) is a practical necessity for training any deep neural network. It will be required between the BDH-GPU blocks to stabilize activations and prevent gradients from vanishing or exploding, ensuring the model can actually learn. The paper's implementation almost certainly uses it. |
| Tier 2 (pending) | oja_hebbian | Both | The model's core.
| Tier 3 (pending) | GeLU | Both | Better ReLU with inhibitory action.