BDH-GPU Kernel Manifest (generated by Gemini)
| Priority | Kernel Name | Forward/Backward | Rationale for BDH-GPU |
| :--- | :--- | :--- | :--- |
| Tier 1 (done) | embedding | Both | The Input Gateway. This is the non-negotiable first layer of the model. It maps input token IDs to their vector representations, which become the initial X vector in the BDH-GPU update loop. Its backward pass (scatter_add) is essential for training the token embeddings. |
| Tier 1 (pending) | crossentropy_loss | Both | The Learning Engine. This is how the model calculates its error and begins the backpropagation process. For numerical stability, this must be a fused kernel that combines log_softmax with the negative log-likelihood calculation. |
| Tier 2 (pending) | softmax | Both | The Output Projector. This is required as the final layer before the loss function to convert the model's output logits into a valid probability distribution over the vocabulary for next-token prediction. It must be a fused, numerically stable kernel. |
| Tier 2 (pending) | layernorm | Both | The Stability
 Engine. While not the central theoretical component of the BDH-GPU's recurrent loop, Layer Normalization (or a similar normalization scheme) is a practical necessity for training any deep neural network. It will be required between the BDH-GPU blocks to stabilize activations and prevent gradients from vanishing or exploding, ensuring the model can actually learn. The paper's implementation almost certainly uses it. |
| Tier 3 (pending) | transpose | Forward Only | The Efficiency Multiplier. The BDH-GPU's core Hebbian update is YX^T, which is an outer product that can be implemented as a matmul involving transposes. A highly optimized kernel for transposing tensors in-memory without significant performance penalty is critical for making this core component fast. |
| Tier 4 (pending) | hebbian_update | Both | This is probably good for speed and not that hard to implement (its just YX^T)